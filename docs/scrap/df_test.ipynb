{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769b52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e023dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/cheemyubuntu/Documents/Projects/LLMS/Attorney-Case-Expert/data/raw/all_opinions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7015a8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author_name', 'category', 'per_curiam', 'case_name', 'date_filed',\n",
       "       'federal_cite_one', 'absolute_url', 'cluster', 'year_filed', 'scdb_id',\n",
       "       'scdb_decision_direction', 'scdb_votes_majority', 'scdb_votes_minority',\n",
       "       'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7fe1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>category</th>\n",
       "      <th>per_curiam</th>\n",
       "      <th>case_name</th>\n",
       "      <th>date_filed</th>\n",
       "      <th>federal_cite_one</th>\n",
       "      <th>absolute_url</th>\n",
       "      <th>cluster</th>\n",
       "      <th>year_filed</th>\n",
       "      <th>scdb_id</th>\n",
       "      <th>scdb_decision_direction</th>\n",
       "      <th>scdb_votes_majority</th>\n",
       "      <th>scdb_votes_minority</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justice Roberts</td>\n",
       "      <td>majority</td>\n",
       "      <td>False</td>\n",
       "      <td>McCutcheon v. Federal Election Comm'n</td>\n",
       "      <td>2014-04-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.courtlistener.com/opinion/2659301/...</td>\n",
       "      <td>https://www.courtlistener.com/api/rest/v3/clus...</td>\n",
       "      <td>2014</td>\n",
       "      <td>2013-033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>There is no right more basic in our democracy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_name  category  per_curiam  \\\n",
       "0  Justice Roberts  majority       False   \n",
       "\n",
       "                               case_name  date_filed federal_cite_one  \\\n",
       "0  McCutcheon v. Federal Election Comm'n  2014-04-02              NaN   \n",
       "\n",
       "                                        absolute_url  \\\n",
       "0  https://www.courtlistener.com/opinion/2659301/...   \n",
       "\n",
       "                                             cluster  year_filed   scdb_id  \\\n",
       "0  https://www.courtlistener.com/api/rest/v3/clus...        2014  2013-033   \n",
       "\n",
       "   scdb_decision_direction  scdb_votes_majority  scdb_votes_minority  \\\n",
       "0                      1.0                  5.0                  4.0   \n",
       "\n",
       "                                                text  \n",
       "0  There is no right more basic in our democracy ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb5df17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_summary(device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07852c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheemyubuntu/miniconda3/envs/ada/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    aten::empty_strided         0.17%      17.332ms         2.65%     278.161ms      65.931us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      58.10 Gb      58.10 Gb          4219  \n",
      "                                              aten::mul         0.14%      14.917ms         0.26%      27.760ms      10.718us       5.123ms         0.05%       5.123ms       1.978us           0 b           0 b      39.30 Mb      39.30 Mb          2590  \n",
      "                                               aten::mm         0.19%      19.813ms         0.30%      31.153ms      27.594us     179.274ms         1.91%     179.274ms     158.790us           0 b           0 b      38.70 Mb      38.70 Mb          1129  \n",
      "                                             aten::sort         0.01%     717.111us         0.22%      23.614ms       2.361ms     438.017us         0.00%     455.392us      45.539us           0 b           0 b      17.40 Mb      17.40 Mb            10  \n",
      "                                             aten::silu         0.02%       2.556ms         0.10%      10.034ms      35.834us     552.510us         0.01%     552.510us       1.973us           0 b           0 b      15.18 Mb      15.18 Mb           280  \n",
      "                                            aten::empty         0.02%       2.562ms         0.03%       2.742ms       2.076us       0.000us         0.00%       0.000us       0.000us       4.16 Kb       4.16 Kb      11.79 Mb      11.79 Mb          1321  \n",
      "                                              aten::pow         0.06%       5.834ms         0.25%      25.751ms      45.177us     714.344us         0.01%     714.344us       1.253us           0 b           0 b      11.69 Mb      11.69 Mb           570  \n",
      "                                         aten::_softmax         0.00%     134.450us         0.10%      10.285ms     514.240us       1.325ms         0.01%       1.325ms      66.228us           0 b           0 b      11.60 Mb      11.60 Mb            20  \n",
      "                                              aten::add         0.11%      11.070ms         0.18%      18.677ms      10.922us       2.452ms         0.03%       2.452ms       1.434us           0 b           0 b       9.31 Mb       9.31 Mb          1710  \n",
      "                                              aten::cat         0.07%       7.718ms         0.17%      17.578ms      15.286us       3.019ms         0.03%       3.019ms       2.625us           0 b           0 b       9.04 Mb       9.04 Mb          1150  \n",
      "                                              aten::bmm         0.05%       5.013ms         0.27%      28.847ms       2.622ms       1.282ms         0.01%       1.282ms     116.569us           0 b           0 b       8.42 Mb       8.42 Mb            11  \n",
      "                                          aten::scatter         0.02%       1.708ms         0.02%       2.399ms     119.947us      52.833us         0.00%      84.961us       4.248us           0 b           0 b       7.25 Mb       7.25 Mb            20  \n",
      "                                           aten::cumsum         0.01%     579.224us         0.08%       8.234ms     392.106us      86.498us         0.00%      86.498us       4.119us           0 b           0 b       5.81 Mb       5.81 Mb            21  \n",
      "                                              aten::div         0.01%     841.450us         0.06%       5.960ms     198.667us      54.848us         0.00%      54.848us       1.828us           0 b           0 b       5.81 Mb       5.81 Mb            30  \n",
      "                                            aten::addmm         0.17%      17.572ms         0.82%      85.664ms     101.981us      16.291ms         0.17%      16.291ms      19.394us           0 b           0 b       3.69 Mb       3.69 Mb           840  \n",
      "                                              aten::neg         0.03%       3.590ms         0.14%      14.391ms      25.699us       1.189ms         0.01%       1.189ms       2.124us           0 b           0 b       1.64 Mb       1.64 Mb           560  \n",
      "                                               aten::lt         0.01%     801.641us         0.07%       7.574ms     244.327us      56.354us         0.00%      56.354us       1.818us           0 b           0 b       1.46 Mb       1.46 Mb            31  \n",
      "                                               aten::le         0.00%     133.429us         0.00%     169.547us      16.955us      15.904us         0.00%      15.904us       1.590us           0 b           0 b       1.45 Mb       1.45 Mb            10  \n",
      "                                             aten::mean         0.05%       4.990ms         0.11%      11.734ms      20.585us       1.939ms         0.02%       1.939ms       3.402us           0 b           0 b     285.00 Kb     285.00 Kb           570  \n",
      "                                            aten::rsqrt         0.03%       3.223ms         0.16%      16.879ms      29.613us     755.812us         0.01%     755.812us       1.326us           0 b           0 b     285.00 Kb     285.00 Kb           570  \n",
      "                                          aten::resize_         0.00%     114.764us         0.00%     114.764us       2.869us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     120.00 Kb     120.00 Kb            40  \n",
      "                                               aten::eq         0.01%     815.772us         0.08%       8.586ms     209.426us      58.432us         0.00%      58.432us       1.425us           0 b           0 b      20.50 Kb      20.50 Kb            41  \n",
      "                                              aten::any         0.00%     286.351us         0.08%       8.766ms     381.116us      36.928us         0.00%      48.064us       2.090us           0 b           0 b      11.50 Kb      11.50 Kb            23  \n",
      "                                              aten::sub         0.00%     165.871us         0.06%       6.794ms     323.510us      26.656us         0.00%      26.656us       1.269us           0 b           0 b      10.50 Kb      10.50 Kb            21  \n",
      "                                               aten::ge         0.00%     203.112us         0.00%     323.277us      16.164us      30.528us         0.00%      30.528us       1.526us           0 b           0 b      10.00 Kb      10.00 Kb            20  \n",
      "                                             aten::topk         0.03%       3.036ms         0.29%      30.765ms       3.077ms     538.492us         0.01%     538.492us      53.849us           0 b           0 b      10.00 Kb      10.00 Kb            10  \n",
      "                                      aten::bitwise_and         0.01%     536.065us         0.09%       9.248ms     462.391us      31.549us         0.00%      31.549us       1.577us           0 b           0 b      10.00 Kb      10.00 Kb            20  \n",
      "                                      aten::bitwise_not         0.00%     422.319us         0.00%     518.520us      25.926us      24.098us         0.00%      24.098us       1.205us           0 b           0 b      10.00 Kb      10.00 Kb            20  \n",
      "                                       aten::bitwise_or         0.00%     468.081us         0.01%     544.646us      27.232us      23.840us         0.00%      23.840us       1.192us           0 b           0 b      10.00 Kb      10.00 Kb            20  \n",
      "                                            aten::index         0.02%       1.693ms         0.13%      14.019ms     737.859us      51.233us         0.00%      51.233us       2.696us           0 b           0 b       9.50 Kb       9.50 Kb            19  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 10.504s\n",
      "Self CUDA time total: 9.410s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"sdpa\"\n",
    ").eval()\n",
    "\n",
    "prompt = \"Say hello in one sentence.\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.inference_mode(), profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    profile_memory=True,           # <-- tracks CUDA mem\n",
    "    record_shapes=True,\n",
    "    with_stack=False\n",
    ") as prof:\n",
    "    _ = model.generate(**inputs, max_new_tokens=64, use_cache=True, do_sample=True, num_beams=1)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=30))\n",
    "# Tip: also try sort_by=\"cuda_memory_usage\" and \"self_cpu_memory_usage\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f9bd5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 30.08it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dtype: torch.bfloat16\n",
      "Prompt tokens: 15\n",
      "[after load + tokenize] allocated=9.42 GiB reserved=9.43 GiB peak=9.42 GiB\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   9644 MiB |   9644 MiB |  78796 MiB |  69151 MiB |\n",
      "|       from large pool |   9644 MiB |   9644 MiB |  77739 MiB |  68095 MiB |\n",
      "|       from small pool |      0 MiB |      0 MiB |   1056 MiB |   1056 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   9644 MiB |   9644 MiB |  78796 MiB |  69151 MiB |\n",
      "|       from large pool |   9644 MiB |   9644 MiB |  77739 MiB |  68095 MiB |\n",
      "|       from small pool |      0 MiB |      0 MiB |   1056 MiB |   1056 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   9644 MiB |   9644 MiB |  78624 MiB |  68980 MiB |\n",
      "|       from large pool |   9643 MiB |   9643 MiB |  77569 MiB |  67925 MiB |\n",
      "|       from small pool |      0 MiB |      0 MiB |   1055 MiB |   1054 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   9660 MiB |   9660 MiB |  39184 MiB |  29524 MiB |\n",
      "|       from large pool |   9656 MiB |   9656 MiB |  39036 MiB |  29380 MiB |\n",
      "|       from small pool |      4 MiB |      4 MiB |    148 MiB |    144 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  15862 KiB |  15862 KiB |   9780 MiB |   9765 MiB |\n",
      "|       from large pool |  12160 KiB |  12160 KiB |   8711 MiB |   8699 MiB |\n",
      "|       from small pool |   3702 KiB |   3702 KiB |   1069 MiB |   1065 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     212    |     212    |   14899    |   14687    |\n",
      "|       from large pool |     122    |     122    |     933    |     811    |\n",
      "|       from small pool |      90    |      90    |   13966    |   13876    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     212    |     212    |   14899    |   14687    |\n",
      "|       from large pool |     122    |     122    |     933    |     811    |\n",
      "|       from small pool |      90    |      90    |   13966    |   13876    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       5    |       5    |     416    |     411    |\n",
      "|       from large pool |       3    |       3    |     342    |     339    |\n",
      "|       from small pool |       2    |       2    |      74    |      72    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       4    |       4    |    6713    |    6709    |\n",
      "|       from large pool |       1    |       1    |     223    |     222    |\n",
      "|       from small pool |       3    |       3    |    6490    |    6487    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Gen time: 15.42s\n",
      "You are Qwen. Answer briefly: What is the capital of France? The capital of France is Paris. Brief answer: Paris. ...\n",
      "[after generate] allocated=9.42 GiB reserved=10.45 GiB peak=10.44 GiB\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   9644 MiB |  10688 MiB | 256169 MiB | 246524 MiB |\n",
      "|       from large pool |   9644 MiB |  10684 MiB | 253657 MiB | 244013 MiB |\n",
      "|       from small pool |      0 MiB |      6 MiB |   2511 MiB |   2511 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   9644 MiB |  10688 MiB | 256169 MiB | 246524 MiB |\n",
      "|       from large pool |   9644 MiB |  10684 MiB | 253657 MiB | 244013 MiB |\n",
      "|       from small pool |      0 MiB |      6 MiB |   2511 MiB |   2511 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   9644 MiB |  10687 MiB | 255443 MiB | 245799 MiB |\n",
      "|       from large pool |   9643 MiB |  10683 MiB | 252934 MiB | 243290 MiB |\n",
      "|       from small pool |      0 MiB |      6 MiB |   2509 MiB |   2508 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  10704 MiB |  10704 MiB | 118962 MiB | 108258 MiB |\n",
      "|       from large pool |  10696 MiB |  10696 MiB | 118804 MiB | 108108 MiB |\n",
      "|       from small pool |      8 MiB |      8 MiB |    158 MiB |    150 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  15862 KiB |  18796 KiB |  14935 MiB |  14919 MiB |\n",
      "|       from large pool |  12160 KiB |  13696 KiB |  12404 MiB |  12392 MiB |\n",
      "|       from small pool |   3702 KiB |   5115 KiB |   2530 MiB |   2527 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     213    |     296    |   36635    |   36422    |\n",
      "|       from large pool |     122    |     125    |    3533    |    3411    |\n",
      "|       from small pool |      91    |     174    |   33102    |   33011    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     213    |     296    |   36635    |   36422    |\n",
      "|       from large pool |     122    |     125    |    3533    |    3411    |\n",
      "|       from small pool |      91    |     174    |   33102    |   33011    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |       8    |    1526    |    1518    |\n",
      "|       from large pool |       4    |       4    |    1447    |    1443    |\n",
      "|       from small pool |       4    |       4    |      79    |      75    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       5    |      26    |   16420    |   16415    |\n",
      "|       from large pool |       1    |       2    |     951    |     950    |\n",
      "|       from small pool |       4    |      25    |   15469    |   15465    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, gc, time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # adjust\n",
    "PROMPT = \"You are Qwen. Answer briefly: What is the capital of France?\"\n",
    "\n",
    "def bytes2gb(x): return f\"{x/1024**3:.2f} GiB\"\n",
    "\n",
    "def report(where):\n",
    "    if not torch.cuda.is_available(): \n",
    "        print(f\"[{where}] CUDA not available\"); return\n",
    "    torch.cuda.synchronize()\n",
    "    alloc = torch.cuda.memory_allocated()\n",
    "    rsvd  = torch.cuda.memory_reserved()\n",
    "    peak  = torch.cuda.max_memory_allocated()\n",
    "    print(f\"[{where}] allocated={bytes2gb(alloc)} reserved={bytes2gb(rsvd)} peak={bytes2gb(peak)}\")\n",
    "    print(torch.cuda.memory_summary())\n",
    "\n",
    "# allocator hints to reduce fragmentation (set once per session if you like)\n",
    "# import os; os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256,expandable_segments:True\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.bfloat16,             # or torch.float16 on 3080 Ti\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",       # or \"flash_attention_2\" if installed\n",
    "    low_cpu_mem_usage=True,\n",
    ").eval()\n",
    "\n",
    "print(\"Model dtype:\", next(model.parameters()).dtype)\n",
    "inputs = tok(PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "print(\"Prompt tokens:\", inputs[\"input_ids\"].shape[-1])\n",
    "\n",
    "# start fresh counters\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "report(\"after load + tokenize\")\n",
    "\n",
    "try:\n",
    "    with torch.inference_mode():\n",
    "        t0 = time.time()\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            num_beams=1,               # beams>1 multiplies KV cache\n",
    "            use_cache=True,\n",
    "            max_new_tokens=256,        # keep modest\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "            output_attentions=False,\n",
    "        )\n",
    "    dt = time.time() - t0\n",
    "    print(\"Gen time:\", f\"{dt:.2f}s\")\n",
    "    print(tok.decode(out[0], skip_special_tokens=True)[:200], \"...\")\n",
    "    report(\"after generate\")\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(\">>> Caught OOM:\", e)\n",
    "    # Deep allocator snapshot to inspect later\n",
    "    try:\n",
    "        torch.cuda.memory._dump_snapshot(\"/tmp/torch_cuda_mem.json\")\n",
    "        print(\"Wrote /tmp/torch_cuda_mem.json (open in a trace viewer).\")\n",
    "    except Exception as ee:\n",
    "        print(\"Snapshot failed:\", ee)\n",
    "    report(\"after OOM\")\n",
    "    # optional cleanup if you want to retry lighter settings\n",
    "    # del out if 'out' in locals() else None\n",
    "    # del model; gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a90f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
